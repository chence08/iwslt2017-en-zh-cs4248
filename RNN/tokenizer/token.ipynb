{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_file, vocab_size, model_name, model_type, character_coverage):\n",
    "    \"\"\"\n",
    "    search on https://github.com/google/sentencepiece/blob/master/doc/options.md to learn more about the parameters\n",
    "    :param input_file: one-sentence-per-line raw corpus file. No need to run tokenizer, normalizer or preprocessor.\n",
    "                       By default, SentencePiece normalizes the input with Unicode NFKC.\n",
    "                       You can pass a comma-separated list of files.\n",
    "    :param vocab_size: vocabulary size, e.g., 8000, 16000, or 32000\n",
    "    :param model_name: output model name prefix. <model_name>.model and <model_name>.vocab are generated.\n",
    "    :param model_type: model type. Choose from unigram (default), bpe, char, or word.\n",
    "                       The input sentence must be pretokenized when using word type.\n",
    "    :param character_coverage: amount of characters covered by the model, good defaults are: 0.9995 for languages with\n",
    "                               rich character set like Japanse or Chinese and 1.0 for other languages with\n",
    "                               small character set.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --model_type=%s --character_coverage=%s ' \\\n",
    "                     '--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 '\n",
    "    cmd = input_argument % (input_file, model_name, vocab_size, model_type, character_coverage)\n",
    "    spm.SentencePieceTrainer.Train(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/haolong/y4s2/cs428/project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"/Users/haolong/y4s2/cs428/chn.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m sp \u001b[39m=\u001b[39m spm\u001b[39m.\u001b[39mSentencePieceProcessor()\n\u001b[1;32m     17\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m美国总统特朗普今日抵达夏威夷。\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m sp\u001b[39m.\u001b[39;49mLoad(\u001b[39m\"\u001b[39;49m\u001b[39m/Users/haolong/y4s2/cs428/chn.model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(sp\u001b[39m.\u001b[39mEncodeAsPieces(text))\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(sp\u001b[39m.\u001b[39mEncodeAsIds(text))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/sentencepiece/__init__.py:905\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39mif\u001b[39;00m model_proto:\n\u001b[1;32m    904\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[0;32m--> 905\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLoadFromFile(model_file)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/sentencepiece/__init__.py:310\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLoadFromFile\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m _sentencepiece\u001b[39m.\u001b[39;49mSentencePieceProcessor_LoadFromFile(\u001b[39mself\u001b[39;49m, arg)\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"/Users/haolong/y4s2/cs428/chn.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "    en_input = '/Users/haolong/y4s2/cs428/corpus.en'\n",
    "    en_vocab_size = 32000\n",
    "    en_model_name = 'eng'\n",
    "    en_model_type = 'bpe'\n",
    "    en_character_coverage = 1\n",
    "    train(en_input, en_vocab_size, en_model_name, en_model_type, en_character_coverage)\n",
    "    \n",
    "    ch_input = '/Users/haolong/y4s2/cs428/corpus.ch'\n",
    "    ch_vocab_size = 32000\n",
    "    ch_model_name = 'chn'\n",
    "    ch_model_type = 'bpe'\n",
    "    ch_character_coverage = 0.9995\n",
    "    train(ch_input, ch_vocab_size, ch_model_name, ch_model_type, ch_character_coverage)\n",
    "    \n",
    "    #test\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    text = \"美国总统特朗普今日抵达夏威夷。\"\n",
    "\n",
    "    \n",
    "    #test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁美国', '总统', '特', '朗', '普', '今日', '抵达', '夏威夷', '。']\n",
      "[985, 1394, 28550, 29531, 29053, 6077, 7772, 8701, 28366]\n",
      "观察到的想象率的回归 并且可以助\n"
     ]
    }
   ],
   "source": [
    "    sp.Load(\"chn.model\")\n",
    "    print(sp.EncodeAsPieces(text))\n",
    "    print(sp.EncodeAsIds(text))\n",
    "    a = [12907, 277, 7419, 7318, 18384, 28724]\n",
    "    print(sp.decode_ids(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "366bd0e816929564b8da31f5ec5d54b0e804cb407beb456598d4b1a5e6c495cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
